{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5794ad43-4546-4d84-82d5-4a0f252d61a1",
   "metadata": {},
   "source": [
    "# Adding malicious into the FL Avg, visualize the effects from malicious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebaa196-8bd3-417f-8f93-59bb1697d5c7",
   "metadata": {},
   "source": [
    "based on FoolsGold's code on malicious\n",
    "https://github.com/DistributedML/FoolsGold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c57c8f-b8dc-4538-9f18-84179ff37dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchmetrics.functional import kl_divergence\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import cm as cm\n",
    "\n",
    "from datetime import datetime \n",
    "from datetime import date\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9056f67c-9053-41e2-824a-5b3ec771611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = os.path.join(DATA_PATH, Path(\"mnist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5a893-19ec-4e16-8e67-8a7e66391d60",
   "metadata": {},
   "source": [
    "## Check the Mnist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecdd797-0698-483b-8352-cf9218270f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\mnist'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9717c2ff-2ed0-40eb-949d-6280bc5dcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'mnist_train.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793ab200-3064-468a-ae7b-eeeed3282541",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = np.load(os.path.join(PATH, Path(file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade06ada-9a74-4c91-ad16-ff6b9a1f3cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4683ef8f-87bf-40ca-a331-f0fe25b65964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.6 , 2.45, 2.5 , 0.66, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.14, 2.54, 2.54, 2.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 2.23, 2.54, 2.54, 2.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.11, 2.54, 2.54, 2.51, 0.77, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.96, 2.54, 2.54, 2.43, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.53, 2.48, 2.54, 2.54, 2.43, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.63, 2.54, 2.54, 2.54, 1.28, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 1.85, 2.54, 2.54, 2.54, 0.58, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.17, 2.54, 2.54, 2.54, 1.83, 0.02, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 1.82, 2.55, 2.54, 2.55, 1.14, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 2.54, 2.54, 2.54, 2.26, 0.16, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.69, 2.54, 2.55, 2.54, 0.6 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 2.45, 2.54, 2.54, 2.2 , 0.23, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.68, 2.54, 2.54, 2.52, 1.14, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.94, 2.54, 2.54, 1.95, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.71, 2.51, 2.54, 2.54, 1.18, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.37, 2.54, 2.54, 1.94, 0.22, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.26, 2.54, 2.54, 0.95, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.26, 2.54, 2.54, 1.97, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.7 , 2.52, 1.35, 0.58, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=np.inf)\n",
    "testdata[1][:784].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba9d3305-0545-4c93-955c-3f8fbeb1fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(testdata[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3991062-20fd-406f-8722-7c831027a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist0.npy (5923, 785)\n",
    "# mnist_bad_1_7.npy (6742, 785) # lable fliping, data(1) label(7)\n",
    "# mnist_uniform_0.npy (6000, 785)\n",
    "# mnist_train.npy (60000,785)\n",
    "# mnist_test.npy (10000, 785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33875bf3-2466-430f-a1af-64208848c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join(PATH, Path(\"mnist_train.npy\")))\n",
    "validation_data = np.load(os.path.join(PATH, Path(\"mnist_test.npy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41bf9c-c0ae-4932-8519-ede404bc1060",
   "metadata": {},
   "source": [
    "## Adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296ca420-3175-40c8-9477-cbd9a104fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clients = 40\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10 # 4\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "if_noniid = True\n",
    "dirichlet_alpha = 0.2\n",
    "num_classes = 10\n",
    "train_amount = 5400\n",
    "random_seed = 0\n",
    "\n",
    "\n",
    "malicious_attack = True\n",
    "number_of_malicious = 10\n",
    "malicious_mode = 'backdoor' # 'untargeted', 'label_flipping', 'backdoor'\n",
    "from_label = 1\n",
    "to_label = 7\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09714724-a849-4dc7-94b2-20562e51cef9",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e549f8-159a-4bdf-b132-ea939719895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary for clients Honest/Malicious:\n",
    "all_clients = {}\n",
    "np.random.seed(random_seed)\n",
    "malicious_names = np.random.choice(range(number_of_clients), number_of_malicious, replace=False)\n",
    "malicious_names.sort()\n",
    "for i in range(number_of_clients):\n",
    "    if i in malicious_names:\n",
    "        all_clients['client'+str(i)] = 'Malicious'\n",
    "    else:\n",
    "        all_clients['client'+str(i)] = 'Honest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6deb207-2106-4894-b499-ae6e4678a238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 11, 15, 18, 20, 22, 25, 28, 29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a19382-378b-4c56-80ae-c397a37cecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create IID or Non-IID dataset for all clients's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "224a549c-c161-4c78-930f-a015ef5245b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_shuffle_labels(y_data, seed, num_classes):\n",
    "    y_data=pd.DataFrame(y_data.cpu(),columns=[\"labels\"]) # create DF on all y values\n",
    "    y_data[\"i\"]=np.arange(len(y_data)) # create a global index\n",
    "    label_dict = dict()\n",
    "    for i in range(num_classes):\n",
    "        var_name=\"label\" + str(i)\n",
    "        label_info=y_data[y_data[\"labels\"]==i] # create an index for each class (only for MNIST)\n",
    "        np.random.seed(seed)\n",
    "        label_info=np.random.permutation(label_info)\n",
    "        label_info=pd.DataFrame(label_info, columns=[\"labels\",\"i\"])\n",
    "        label_dict.update({var_name: label_info })\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c046bf1-14c8-4431-946d-e76d3656d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1]\n",
    "x_valid = validation_data[:, :-1]\n",
    "y_valid = validation_data[:, -1]\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_valid = x_valid.astype(np.float32)\n",
    "# convert y lables to ints\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_valid = y_valid.astype(np.int64)\n",
    "unique, counts = np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b00eb92-907a-45ae-ae7e-0ddb250db600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = map(torch.tensor, (x_train, y_train))\n",
    "x_train = torch.from_numpy(x_train) # .to(device)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_valid = torch.from_numpy(x_valid)\n",
    "y_valid = torch.from_numpy(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eed8cf9-8f35-4ca6-b0f7-0c568faf5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79268d0d-c3e4-48cd-8b26-0a10dc520ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution in train data\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "267f2f73-4eab-4208-8b4b-66fb5f36fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dict_train = split_and_shuffle_labels(y_data=y_train, seed=1, amount=train_amount) \n",
    "def get_subsamples_indices_new(label_dict, number_of_clients, amount, num_classes, NonIID, alpha):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_clients)) # 45\n",
    "    \n",
    "    if NonIID == False:\n",
    "        for i in range(number_of_clients): # create 100 number_of_clients\n",
    "            sample_name=\"sample\"+str(i)\n",
    "            dumb=pd.DataFrame()\n",
    "            # IID distribution, each number class gets same samples\n",
    "            for j in range(num_classes): # for each number 0 - 9\n",
    "                label_name=str(\"label\")+str(j)\n",
    "                a=label_dict[label_name][i*batch_size:(i+1)*batch_size] # get 45 record of one number \n",
    "                dumb=pd.concat([dumb,a], axis=0) # concat total 10 of them 0 - 9\n",
    "            dumb.reset_index(drop=True, inplace=True)    \n",
    "            sample_dict.update({sample_name: dumb}) # each sample 450, 100 samples\n",
    "    else:\n",
    "        # based of dirichlet distribution\n",
    "        #print(\"label_dict\", label_dict)\n",
    "        image_nums = []\n",
    "        sample_dict = {}\n",
    "        \n",
    "        for i in range (number_of_clients):\n",
    "            sample_name=\"sample\"+str(i)\n",
    "            sample_dict.update({sample_name: pd.DataFrame()})\n",
    "        \n",
    "        for label in range (num_classes):\n",
    "            image_num = []\n",
    "            sampled_probabilities = amount * np.random.dirichlet(\n",
    "                np.array(number_of_clients * [alpha]))\n",
    "            class_label_len = len(label_dict[str(\"label\")+str(label)])\n",
    "            # print(\"class_label_len\", class_label_len)\n",
    "            \n",
    "            for sample in range(number_of_clients):\n",
    "                # print(\"sample\", sample)\n",
    "                dumb2 = pd.DataFrame()\n",
    "                no_imgs = int(round(sampled_probabilities[sample]))\n",
    "                label_name=str(\"label\")+str(label)\n",
    "\n",
    "                sampled_list = label_dict[label_name][:min(class_label_len, no_imgs)]\n",
    "                image_num.append(len(sampled_list))\n",
    "                dumb2=pd.concat([dumb2, sampled_list], axis=0)\n",
    "                \n",
    "                class_label_len = class_label_len - len(sampled_list)\n",
    "                # print(\"user\", user)\n",
    "                # print(\"no_imgs\", no_imgs)\n",
    "                # print(\"class_label_len\", class_label_len)\n",
    "                sample_name=\"sample\"+str(sample)\n",
    "                image_nums.append(image_num)\n",
    "                # print(\"dumb2\", dumb2)\n",
    "                # print(\"label_name\", label_name)\n",
    "                # print(\"[dumb[label_name]\", dumb[label_name])\n",
    "                #dumb.reset_index(drop=True, inplace=True)\n",
    "                sample_dict[sample_name] = pd.concat([sample_dict[sample_name], dumb2])\n",
    "        # self.draw_dirichlet_plot(10, number_of_clients,image_nums, alpha)\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94d1f22f-7348-4435-b240-3124f631ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples 100\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        # use the global index to reconnect x and y\n",
    "        indices=np.sort(np.array(sample_dict[sample_name][\"i\"])) \n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fe832-c5a1-41c8-949e-e949a231a8a9",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bfa6802-b1d8-477d-a500-b4c79134d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(784,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e2bd0e1-968e-428b-ae41-15e1223cfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e10160f-8c18-4eb0-bea7-8684c5fea922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d01ba-1abf-45c7-bc5b-9dd24bedda21",
   "metadata": {},
   "source": [
    "## Functions for Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b685e2b1-6167-4928-9a38-061bf3428469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train_end_node_process_without_print(malicious_attack, malicious_names, number_of_clients, sample_list, iteration, plot_indiv, plot_glob, layer, mode, global_table):\n",
    "    initial_result = create_similiarity_matrix(model_dict, layer, 0, mode)\n",
    "    # plot_matrix(initial_result, layer, iteration, mode, 0, file_location=\"Plots/\")\n",
    "    global_table.append([iteration, 0, layer, mode, initial_result])\n",
    "    \n",
    "    for epoch in range(1, numEpoch+1): \n",
    "        for i in range (number_of_clients):\n",
    "            # print(\"client i\", i)\n",
    "            if malicious_attack == True:\n",
    "                if i not in malicious_names:\n",
    "                    train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "                    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                else:\n",
    "                    train_ds = TensorDataset(x_train_m_dict[\"x_train_m_\" + str(i)], y_train_m_dict[\"y_train_m_\" + str(i)]) # malicious dataset\n",
    "                    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                \n",
    "                model=model_dict[name_of_models[i]]\n",
    "                criterion=criterion_dict[name_of_criterions[i]]\n",
    "                optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "\n",
    "                train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            \n",
    "            elif malicious_attack == False:\n",
    "                train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "                train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                \n",
    "                model=model_dict[name_of_models[i]]\n",
    "                criterion=criterion_dict[name_of_criterions[i]]\n",
    "                optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "\n",
    "                train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "                # test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "\n",
    "        if plot_indiv is True:\n",
    "            for sample in sample_list:\n",
    "                plot_node_weights(model_dict, sample[0], sample[1], iteration)\n",
    "\n",
    "        if plot_glob is True:\n",
    "            iteration_result = create_similiarity_matrix(model_dict, layer, iteration, mode)\n",
    "            # plot_matrix(iteration_result, layer, iteration, mode, epoch, file_location=\"Plots/\")\n",
    "            global_table.append([iteration, epoch, layer, mode, iteration_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a85b90c-2d8a-46ba-afb1-c923306d0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_optimizer_criterion_dict(number_of_clients):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_clients):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn()\n",
    "        model_info # .to(device)\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ccce64d-d46a-4e79-898c-6edb74820bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     a = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape).to(device)\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2df91fc-c638-493d-b805-0b85bb96db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_weights(model_dict, number_of_clients):\n",
    "   \n",
    "    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n",
    "    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n",
    "    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n",
    "    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n",
    "            \n",
    "    # fc1_mean_weight.to(device)\n",
    "    # fc1_mean_bias.to(device)\n",
    "    # fc2_mean_weight.to(device)\n",
    "    # fc2_mean_bias.to(device)\n",
    "    # fc3_mean_weight.to(device)\n",
    "    # fc3_mean_bias.to(device)\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_clients):\n",
    "            \n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/number_of_clients\n",
    "        fc1_mean_bias = fc1_mean_bias/ number_of_clients\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/number_of_clients\n",
    "        fc2_mean_bias = fc2_mean_bias/ number_of_clients\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/number_of_clients\n",
    "        fc3_mean_bias = fc3_mean_bias/ number_of_clients\n",
    "        \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31d5af22-2d6d-4864-813b-1b84cd5181df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_clients):\n",
    "    \n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights(model_dict, number_of_clients=number_of_clients)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "037e7f10-a60a-466e-bee0-4042e486d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_clients):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_clients):\n",
    "\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d4cdb-1a26-435b-94a3-b7d92b671b83",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99dda967-8d6a-4551-8c43-749e5e724723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global matrix\n",
    "def plot_matrix(result, layer, iteration, mode, epoch, vmin, vmax, num_ticks, file_location=\"Plots/\"):\n",
    "    matplotlib.use('Agg')\n",
    "    labels = list(range(0, number_of_clients))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,15))\n",
    "    # cax = ax.matshow(result, interpolation='nearest')\n",
    "    if mode == 'cos':\n",
    "        colormap = 'OrRd'\n",
    "    elif mode == 'l2':\n",
    "        colormap = 'YlGnBu'\n",
    "    \n",
    "    cax = ax.imshow(result, vmin = vmin, vmax = v_max, cmap=colormap, interpolation='nearest') # cmap='YlGnBu', 'OrRd'    \n",
    "    # ax.grid(True)\n",
    "    plt.title('FL Clients Similarity Matrix IID' + \" \" + str(mode) + \" \" + \"iter\" + str(iteration) + \"_\" + \"epoch\" + str(epoch) + \"_\" + \"m\" + str(malicious_attack) + \"_\" + malicious_mode, fontsize = 15)\n",
    "    plt.xticks(range(number_of_clients), labels, rotation=90);\n",
    "    plt.yticks(range(number_of_clients), labels);\n",
    "    # ticks = np.linspace(vmin, vmax, num_ticks)\n",
    "    ticks = np.arange(v_min, v_max, round((v_max - v_min)/(num_ticks-1), 2))\n",
    "    ticks = np.append(ticks, v_max)\n",
    "    cbar = fig.colorbar(cax, ax=ax)\n",
    "\n",
    "    font_size = 10 # Adjust as appropriate.\n",
    "    cbar.set_ticks(ticks)\n",
    "    cbar.ax.tick_params(labelsize=font_size)\n",
    "    \n",
    "    # plt.show()\n",
    "    \n",
    "    # create a time stamp\n",
    "    today = date.today().strftime('%Y%m%d')\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H-%M-%S\")\n",
    "    \n",
    "    plt_name = \"Glob\" + \"_\" + layer + \"_\"  + \"I\" + str(iteration) + \"_\" +\\\n",
    "                \"E\" + str(epoch) + \"_\" + str(today) + \"_\" + str(current_time) + \".png\"\n",
    "    \n",
    "    node_folder = file_location + layer + \"_\" + str(mode)\n",
    "    \n",
    "    if not os.path.exists(node_folder):\n",
    "        os.makedirs(node_folder)\n",
    "    \n",
    "    plt.savefig(node_folder + '/' + plt_name)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0350f2d4-bef2-4d85-8bac-59b3b8171cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual node weights and bias\n",
    "def plot_node_weights(model_dict, layer, selected_weight, iteration, file_location=\"Plots/\"):\n",
    "    # create an array of the weight values|\n",
    "    sample_weights = []\n",
    "    sample_bias = []\n",
    "    for i in range (number_of_clients):\n",
    "        current_model = \"model\" + str(i)\n",
    "        w_local = getattr(model_dict[current_model], layer).weight[selected_weight[0],selected_weight[1]].detach().numpy()\n",
    "        b_local = getattr(model_dict[current_model], layer).bias[selected_weight[0]].detach().numpy()\n",
    "        \n",
    "        sample_weights.append(w_local.tolist())\n",
    "        sample_bias.append(b_local.tolist())\n",
    "\n",
    "    num_bins = int(number_of_clients/4)\n",
    "\n",
    "    # process the data with GaussianMixture model\n",
    "    sample_weights = np.asarray(sample_weights)\n",
    "    sample_weights = sample_weights.reshape(len(sample_weights), 1)\n",
    "    sample_bias = np.asarray(sample_bias)\n",
    "    sample_bias = sample_bias.reshape(len(sample_bias), 1)    \n",
    "\n",
    "    # Create a histgram\n",
    "    # fig, ax = plt.subplots()\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharey=False, figsize=(10, 15))\n",
    "\n",
    "    n, bins, patches = ax1.hist(sample_weights, num_bins, \n",
    "                            density = 1, \n",
    "                            color ='darkolivegreen',\n",
    "                            alpha = 0.7)\n",
    "    \n",
    "    mu1 = np.mean(sample_weights)\n",
    "    fig.set_facecolor(\"lightgray\")\n",
    "    ax1.axvline(mu1, color='blue', linestyle='dashed', linewidth=1)\n",
    "    # min_ylim, max_ylim = plt.ylim()\n",
    "    plt.suptitle('IID Layer of: ' + layer + ', Random Weight: ' +  str(selected_weight) + ', Iter ' + str(iteration))\n",
    "    ax1.set_title('Weights Mean: {:.6f}'.format(mu1))\n",
    "    \n",
    "    n, bins, patches = ax2.hist(sample_bias, num_bins, \n",
    "                            density = 1, \n",
    "                            color ='brown',\n",
    "                            alpha = 0.7)\n",
    "    \n",
    "    mu2 = np.mean(sample_bias)\n",
    "    ax2.axvline(mu2, color='blue', linestyle='dashed', linewidth=1)\n",
    "    ax2.set_title('Bias Mean: {:.6f}'.format(mu2))    \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    # create a time stamp\n",
    "    today = date.today().strftime('%Y%m%d')\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H-%M-%S\")\n",
    "    \n",
    "    # plt.show() \n",
    "    plt_name = layer + \"_\" + str(selected_weight[0]) + \"_\" + str(selected_weight[1]) + \"_I_\" + \\\n",
    "    str(iteration) + \"_\" + str(today) + \"_\" + str(current_time) + \".png\"\n",
    "    \n",
    "    node_folder = file_location + layer + \"_\" + str(selected_weight[0]) + \"_\" + str(selected_weight[1])\n",
    "    \n",
    "    if not os.path.exists(node_folder):\n",
    "        os.makedirs(node_folder)\n",
    "    \n",
    "    plt.savefig(node_folder + '/' + plt_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fba59ef-aaef-4820-b52a-8d5915b3ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling based on parameters on layer selection\n",
    "def random_sampling_nodes(model, each_layer_sample_num):\n",
    "    layers = []\n",
    "    for name, layer in model.named_modules():\n",
    "        if name != \"\":\n",
    "            dnn_layer = getattr(model, name)\n",
    "            layers.append([name, [dnn_layer.weight.size()[0], dnn_layer.weight.size()[1]], dnn_layer.bias.size()[0]])\n",
    "    print(*layers,sep='\\n')\n",
    "    \n",
    "    if len(each_layer_sample_num)!=len(layers):\n",
    "        print(\"each_layer_sample_num list's length not equal to model layer list\")\n",
    "        return(-1)\n",
    "    else:\n",
    "        weights_sample = []\n",
    "        for i in range(len(layers)):\n",
    "            for j in range (each_layer_sample_num[i]):\n",
    "                random_weight = [np.random.randint(layers[i][1][0]), np.random.randint(layers[i][1][1])]\n",
    "                print(\"random_weight:\", random_weight)\n",
    "                weights_sample.append([layers[i][0], random_weight])\n",
    "    return weights_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e00c979b-328c-487f-94b3-15f1226aba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances Matrixes\n",
    "def l2dist(p1, p2):\n",
    "    \"\"\"L2 distance between p1, p2, each of which is a tensor\"\"\"\n",
    "    \n",
    "    return torch.dist(p1, p2, p=2)\n",
    "    \n",
    "\n",
    "def cosine_similarity(grad1, grad2, normalized=False):\n",
    "\t\"\"\"\n",
    "\tInput: two sets of gradients of the same shape\n",
    "\tOutput range: [-1, 1]\n",
    "\t\"\"\"\n",
    "\n",
    "\tcos_sim = F.cosine_similarity(torch.flatten(grad1), torch.flatten(grad2), 0, 1e-10) \n",
    "\tif normalized:\n",
    "\t\treturn (cos_sim + 1) / 2.0\n",
    "\telse:\n",
    "\t\treturn cos_sim\n",
    "# KLD relative entripy\n",
    "# kl_divergence(q, p) q, p as tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63b16354-22f2-40f8-8c8a-216384746456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the similiarity matrix of the avg distance or similiarity between clients\n",
    "def create_similiarity_matrix(model_dict, layer, iteration, mode):\n",
    "    # create an array of the weight values\n",
    "    if mode not in [\"l2\", \"cos\"]:\n",
    "        raise Error(\"model has to be in 'l2', 'cos'.\")\n",
    "    \n",
    "    sample_weights = []\n",
    "    for i in range (number_of_clients):\n",
    "        current_model = \"model\" + str(i)\n",
    "        w_i = getattr(model_dict[current_model], layer).weight\n",
    "        sample_weights.append(w_i)\n",
    "    \n",
    "    resultTable = []\n",
    "    for i in range(len(sample_weights)):\n",
    "        distances = []\n",
    "        for j in range(len(sample_weights)):\n",
    "            if mode == \"l2\":\n",
    "                distances.append(l2dist(sample_weights[i], sample_weights[j]).item())\n",
    "            elif mode == \"cos\":\n",
    "                distances.append(cosine_similarity(sample_weights[i], sample_weights[j], False).item())\n",
    "        resultTable.append(distances)\n",
    "        \n",
    "    # print(\"resultTable[0]\", resultTable[0])    \n",
    "    return(resultTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963f8ad-43be-43d0-9bf6-42946cd76b09",
   "metadata": {},
   "source": [
    "## Create the main model and distribute model and data to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ea04eac-9b0b-4a0e-83a6-e21e9aa956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = Net2nn()\n",
    "main_model # .to(device)\n",
    "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b7d9f09-a57b-46cd-a70c-6614b371fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57ca9226-96d0-4f00-b153-f416589fc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_train=split_and_shuffle_labels(y_data=y_train, seed=1, num_classes = num_classes)\n",
    "sample_dict_train = get_subsamples_indices_new(label_dict=label_dict_train, number_of_clients=number_of_clients, amount=train_amount, num_classes=num_classes, NonIID=True, alpha = dirichlet_alpha)\n",
    "x_train_dict, y_train_dict = create_subsamples(sample_dict=sample_dict_train, x_data=x_train, y_data=y_train, x_name=\"x_train\", y_name=\"y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dbe48fb-684a-4212-9202-4edb7b4b401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dict_train['label7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2be44762-94a0-4556-83ec-f68854265fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    757\n",
       "4     76\n",
       "8     67\n",
       "7     66\n",
       "0     52\n",
       "6     32\n",
       "5     21\n",
       "3      2\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict_train['sample0']['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81b7f6cd-1bca-486c-845f-a348176239aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>26028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>41173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>53527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>14768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>8</td>\n",
       "      <td>56666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8</td>\n",
       "      <td>19219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>8</td>\n",
       "      <td>23515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>8</td>\n",
       "      <td>13998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>8</td>\n",
       "      <td>51634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1073 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    labels      i\n",
       "0        0   3144\n",
       "1        0  26028\n",
       "2        0  41173\n",
       "3        0  53527\n",
       "4        0  14768\n",
       "..     ...    ...\n",
       "62       8  56666\n",
       "63       8  19219\n",
       "64       8  23515\n",
       "65       8  13998\n",
       "66       8  51634\n",
       "\n",
       "[1073 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict_train['sample0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b55b3c74-3ce4-4623-aa37-50e912a26f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_dict['x_train0'][1].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7274d26-8db4-4e9d-9ab1-39c437ef5ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 11, 15, 18, 20, 22, 25, 28, 29])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb917b33-c482-4b6e-abdc-49608239358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for malicious clients, training with malicious data\n",
    "def create_malicious_datasets(x_train_dict, y_train_dict, malicious_names, mode, from_label, to_label):\n",
    "    x_train_m_dict = {}\n",
    "    y_train_m_dict = {}\n",
    "    \n",
    "    if mode == 'untargeted':\n",
    "        for i in malicious_names:\n",
    "            # keep x train unchanged\n",
    "            x_train_m_dict[\"x_train_m_\"+str(i)] = x_train_dict[\"x_train\"+str(i)]\n",
    "            y_untargeted = []\n",
    "            for j in range(len(y_train_dict['y_train'+str(i)])):\n",
    "                # y = existed y label + 1, 1->2, ... , 9 -> 1\n",
    "                y_untargeted.append((y_train_dict['y_train'+ str(i)][j].cpu()  + 1) % 10)\n",
    "            y_train_m_dict[\"y_train_m_\"+str(i)] = torch.from_numpy(np.asarray(y_untargeted)) # .to(device)\n",
    "    \n",
    "    if mode == 'label_flipping':\n",
    "        for i in malicious_names:\n",
    "            # keep x train unchanged\n",
    "            x_train_m_dict[\"x_train_m_\"+str(i)] = x_train_dict[\"x_train\"+str(i)]\n",
    "            y_flipped = []\n",
    "            for j in range(len(y_train_dict['y_train'+str(i)])):\n",
    "                # if y is from_label, update it to to_label, vice versa\n",
    "                current_label = y_train_dict['y_train'+ str(i)][j].cpu()\n",
    "                if current_label not in [from_label, to_label]:\n",
    "                    y_flipped.append(current_label)\n",
    "                else:\n",
    "                    if current_label == from_label:\n",
    "                        y_flipped.append(to_label)\n",
    "                    elif current_label == to_label:\n",
    "                        y_flipped.append(from_label)\n",
    "            y_train_m_dict[\"y_train_m_\"+str(i)] = torch.from_numpy(np.asarray(y_flipped)) # .to(device)\n",
    "    \n",
    "    if mode == 'backdoor':\n",
    "        for i in malicious_names:\n",
    "            client_x = []\n",
    "            client_y = []\n",
    "            for j in range(len(x_train_dict['x_train'+str(i)])):\n",
    "                current_label = y_train_dict['y_train'+ str(i)][j]\n",
    "                current_x = x_train_dict['x_train'+ str(i)][j]\n",
    "                if current_label == from_label:\n",
    "                    # Set pixel #783 to white (the brightest white in picture)                    \n",
    "                    current_x[783] = torch.max(current_x)\n",
    "                    client_x.append(current_x)\n",
    "                    client_y.append(to_label)\n",
    "                else:\n",
    "                    client_x.append(current_x)\n",
    "                    client_y.append(current_label.item())\n",
    "            client_x = torch.stack(client_x)\n",
    "            x_train_m_dict[\"x_train_m_\"+str(i)] = client_x # .to(device)\n",
    "            y_train_m_dict[\"y_train_m_\"+str(i)] = torch.from_numpy(np.asarray(client_y).astype(np.int64)) # .to(device)\n",
    "    return (x_train_m_dict, y_train_m_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03dbd4e3-d16c-4002-835c-e2cad3b765ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the create_malicious_dataset function\n",
    "# x_train_m_dict, y_train_m_dict = create_malicious_datasets(x_train_dict, y_train_dict, malicious_names, 'backdoor', 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9ed0cb2-8a9d-454c-b094-66b184838c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_m_dict['x_train_m_4'][4].numpy().reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ddad6bf-23f6-4efb-b93a-80e8973dc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_m_dict['y_train_m_4'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0420b9a8-cbce-40ff-9b94-2a75ee780146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(y_train_m_dict['y_train_m_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50487961-bbcc-4dbd-ae2e-0f573d18a77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'backdoor'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3334104-7c50-453f-a5a8-1c375ad1eb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_train0': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train1': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train2': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train3': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train4': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train5': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train6': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train7': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train8': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train9': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train10': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train11': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train12': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train13': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train14': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train15': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train16': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train17': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train18': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train19': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train20': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train21': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train22': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train23': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train24': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train25': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train26': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train27': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train28': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train29': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train30': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train31': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train32': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train33': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train34': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train35': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train36': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train37': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train38': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'x_train39': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "805a6de6-1c5a-4c16-9f86-b1bfc43bae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the malicious datasets\n",
    "x_train_m_dict, y_train_m_dict = create_malicious_datasets(x_train_dict, y_train_dict, malicious_names, malicious_mode, from_label, to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da7afaf8-a4a2-443c-8e13-03c9ca157aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_m_dict['y_train_m_4'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0c6c40f-fadf-4f01-b349-c022d30a727b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.18, 2.32, 0.42, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 1.48, 2.53, 2.55, 2.31, 0.43, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.35, 1.48, 2.46, 2.54, 2.54, 2.54, 2.54, 0.91, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.33, 2.42, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 0.91, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.33, 2.51, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 2.54, 0.91, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 2.34, 2.54, 2.54, 2.28, 1.29, 0.84, 2.54, 2.54, 2.54, 0.91, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 2.18, 2.54, 2.54, 2.28, 0.54, 0.23, 1.92, 2.54, 2.54, 2.54, 0.91, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.06, 2.54, 2.54, 2.54, 1.35, 0.64, 1.92, 2.54, 2.54, 2.54, 2.54, 1.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.06, 2.54, 2.54, 2.1 , 1.22, 2.44, 2.54, 2.54, 2.54, 2.54, 2.54, 2.48, 0.78, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.06, 2.54, 2.54, 2.54, 2.54, 2.54, 2.55, 2.54, 2.21, 2.54, 2.54, 2.54, 0.98, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.81, 2.49, 2.54, 2.54, 2.54, 2.54, 2.39, 0.98, 0.32, 1.97, 2.54, 2.54, 0.98, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 2.02, 2.54, 2.54, 2.54, 1.75, 0.53, 0.  , 0.  , 1.77, 2.54, 2.54, 1.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 1.25, 1.64, 0.78, 0.16, 0.  , 0.  , 0.  , 1.77, 2.54, 2.54, 2.28, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.77, 2.54, 2.54, 2.28, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.77, 2.54, 2.54, 2.11, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.77, 2.54, 2.54, 1.78, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 2.54, 2.54, 2.28, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.47, 2.54, 2.54, 2.28, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.47, 2.54, 2.54, 2.28, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 2.11, 2.54, 1.6 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(x_train_m_dict['x_train_m_4'][20].cpu()).reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f93dd15-6d69-437a-8d94-bc80406cb659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([247, 784])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dict[\"x_train1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc0457e8-79d1-4ce9-b58f-e9d4e87cf9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0074, -0.0407,  0.0398, -0.0501,  0.0021]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0304,  0.0056, -0.0284, -0.0008, -0.0305]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "feca4ad0-b911-44c0-9d83-c72ae09f4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_x_train_sets = list(x_train_dict.keys())\n",
    "name_of_y_train_sets = list(y_train_dict.keys())\n",
    "name_of_x_train_m_sets = list(x_train_m_dict.keys())\n",
    "name_of_y_train_m_sets = list(y_train_m_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f83e7511-392b-4978-bd3f-fc4abdba5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f3adefc-85c2-46dd-a3d4-9f3073b04800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0074, -0.0407,  0.0398, -0.0501,  0.0021]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0074, -0.0407,  0.0398, -0.0501,  0.0021]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# After initialization the clients' models are the same as main model\n",
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172ccca-ffec-49d2-835b-5eeaa8ed7ced",
   "metadata": {},
   "source": [
    "### Select nodes samples for individual node's visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7e7ce115-9431-40c0-963e-2363c7e501a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_individual = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af3f4eff-217a-4a0c-928a-2090d78ae4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fc1', [200, 784], 200]\n",
      "['fc2', [200, 200], 200]\n",
      "['fc3', [10, 200], 10]\n",
      "random_weight: [7, 125]\n"
     ]
    }
   ],
   "source": [
    "each_layer_list = [0,0,1]\n",
    "sample_t = random_sampling_nodes(main_model, each_layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f289987-714c-4f23-8bff-1ebf02078c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fc3', [7, 125]]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f9051-5e43-4733-b445-75eb4aee9ab6",
   "metadata": {},
   "source": [
    "### Select layer and the distance measurement matrix for global visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65c5fd3e-dc04-47d9-bc3e-e0a6baec1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd16fb68-bd0b-490e-99e1-68b4279b8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_layer = 'fc3'\n",
    "distance_matrix = 'cos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca00e67-dc53-413d-a2f6-83cc2c79ea12",
   "metadata": {},
   "source": [
    "## Training and malicious training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2383c6a9-ff8b-4cf9-a9b5-d2dd21d636a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_records = []\n",
    "test_accuracy_records = []\n",
    "global_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0c394db-e9b1-434e-aea0-5d6876226f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_m_dict[\"x_train_m_4\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e71c3c88-b81d-4b8f-8d0a-dd71fcc164e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ce642b2-559e-4ed0-aeba-4f4425424eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset(x_train_m_dict[\"x_train_m_4\"], y_train_m_dict[\"y_train_m_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c8daa60a-653d-45f4-8414-ac42fde8ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 : main_model accuracy on all test data:  0.7141\n",
      "Iteration 2 : main_model accuracy on all test data:  0.8691\n",
      "Iteration 3 : main_model accuracy on all test data:  0.9008\n",
      "Iteration 4 : main_model accuracy on all test data:  0.9123\n",
      "Iteration 5 : main_model accuracy on all test data:  0.9194\n",
      "Iteration 6 : main_model accuracy on all test data:  0.9253\n",
      "Iteration 7 : main_model accuracy on all test data:  0.9291\n",
      "Iteration 8 : main_model accuracy on all test data:  0.9350\n",
      "Iteration 9 : main_model accuracy on all test data:  0.9368\n",
      "Iteration 10 : main_model accuracy on all test data:  0.9409\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_clients)\n",
    "    start_train_end_node_process_without_print(malicious_attack, malicious_names, number_of_clients, sample_t, i+1, plot_individual, plot_global, weights_layer, distance_matrix, global_results)\n",
    "    \n",
    "    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_clients)\n",
    "    \n",
    "    test_loss, test_accuracy = validation(main_model, valid_dl, main_criterion)\n",
    "    test_loss_records.append(test_loss)\n",
    "    test_accuracy_records.append(test_accuracy)\n",
    "    print(\"Iteration\", str(i+1), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d806e49-15e6-44a9-8e19-67e7fa28092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IID, no attack\n",
    "# Iteration 1 : main_model accuracy on all test data:  0.9254\n",
    "# Iteration 2 : main_model accuracy on all test data:  0.9429\n",
    "# Iteration 3 : main_model accuracy on all test data:  0.9508\n",
    "# Iteration 4 : main_model accuracy on all test data:  0.9567\n",
    "# Iteration 5 : main_model accuracy on all test data:  0.9604\n",
    "# Iteration 6 : main_model accuracy on all test data:  0.9641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "713ded8a-595c-4063-82b3-acbf82e8b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IID, untargeted attack\n",
    "# Iteration 1 : main_model accuracy on all test data:  0.9010\n",
    "# Iteration 2 : main_model accuracy on all test data:  0.8226\n",
    "# Iteration 3 : main_model accuracy on all test data:  0.8837\n",
    "# Iteration 4 : main_model accuracy on all test data:  0.8530\n",
    "# Iteration 5 : main_model accuracy on all test data:  0.9061\n",
    "# Iteration 6 : main_model accuracy on all test data:  0.8478\n",
    "# Iteration 7 : main_model accuracy on all test data:  0.9223\n",
    "# Iteration 8 : main_model accuracy on all test data:  0.7832\n",
    "# Iteration 9 : main_model accuracy on all test data:  0.9437\n",
    "# Iteration 10 : main_model accuracy on all test data:  0.6565"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4acfec3-8d68-4dd4-97d3-b67a1fe195fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ploting and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d26759-0851-4199-8262-e0f4cd0e9725",
   "metadata": {},
   "source": [
    "### Ploting Global Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4069c311-c47c-4e09-8783-45ac7c7ad187",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_master = []\n",
    "for i in range(len(global_results)):\n",
    "    matrix_master.append(global_results[i][-1])\n",
    "    \n",
    "matrix_master = np.asarray(matrix_master)\n",
    "v_min = np.amin(matrix_master)\n",
    "v_max = np.amax(matrix_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66c724e4-0974-4468-8f78-d9237fbf1cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4978605806827545, 1.000000238418579)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v_min, v_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9b47ac0-6502-4336-b650-721c4f8619ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(global_results)):\n",
    "    iteration = global_results[i][0]\n",
    "    epoch = global_results[i][1]\n",
    "    layer = global_results[i][2]\n",
    "    mode = global_results[i][3]\n",
    "    result_matrix = global_results[i][-1]\n",
    "    \n",
    "    plot_matrix(result_matrix, layer, iteration, mode, epoch, v_min, v_max, num_ticks=9, file_location=\"Plots/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2ab7d-bf18-452b-bb3b-b755fa4a3477",
   "metadata": {},
   "source": [
    "### Ploting Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7baa3385-dea1-47f9-a04d-f0079322d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location=\"Plots/\"\n",
    "if malicious_attack is False:\n",
    "    node_folder = file_location + weights_layer + \"_\" + str(distance_matrix) # + \"_\" + \"m\" + str(malicious_attack)\n",
    "else:\n",
    "    node_folder = file_location + weights_layer + \"_\" + str(distance_matrix) # + \"_\" + \"m\" + str(malicious_attack) + \"_\" + malicious_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8bd693f8-83f7-4b84-8931-5169a7f2ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot lines of loss and accuracy\n",
    "plt.clf()\n",
    "plt.plot(test_accuracy_records)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(['Validation'], loc='upper left')\n",
    "plt_name = 'Test_Accuracy_Records'\n",
    "plt.savefig(node_folder + '/' + plt_name)\n",
    "\n",
    "# summarize history for loss\n",
    "plt.clf()\n",
    "plt.plot(test_loss_records, color='orange')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(['Validation'], loc='upper left')\n",
    "plt_name = 'Test_Loss_Records'\n",
    "plt.savefig(node_folder + '/' + plt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a3de6-72d3-4705-a4a8-08b914dde264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705eaa73-4379-437a-80e1-58fecd28e3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb019061-d995-4d7b-a7fa-a796742a08f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b30b34-938c-4d2f-9e8f-24d2eae29af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6352e9-fb11-4043-a358-4ffdc7129792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff83c8-b0e8-47e3-9a85-5fd927b41a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cb272-a94a-4f7e-93a5-23713f03ca2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707fb8c2-1879-41ea-a061-3156f66850b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3a50e-8d6c-4d6b-ae42-4a0d549089ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcdc10-1270-46d1-abe0-3d7b272a3d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
