{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5794ad43-4546-4d84-82d5-4a0f252d61a1",
   "metadata": {},
   "source": [
    "# Adding malicious into the FL Avg, visualize the effects from malicious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebaa196-8bd3-417f-8f93-59bb1697d5c7",
   "metadata": {},
   "source": [
    "based on FoolsGold's code on malicious\n",
    "https://github.com/DistributedML/FoolsGold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c57c8f-b8dc-4538-9f18-84179ff37dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchmetrics.functional import kl_divergence\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9056f67c-9053-41e2-824a-5b3ec771611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"data\")\n",
    "PATH = os.path.join(DATA_PATH, Path(\"mnist\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5a893-19ec-4e16-8e67-8a7e66391d60",
   "metadata": {},
   "source": [
    "## Check the Mnist Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecdd797-0698-483b-8352-cf9218270f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\mnist'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9717c2ff-2ed0-40eb-949d-6280bc5dcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'mnist_bad_1_7.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793ab200-3064-468a-ae7b-eeeed3282541",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = np.load(os.path.join(PATH, Path(file_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade06ada-9a74-4c91-ad16-ff6b9a1f3cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6742, 785)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4683ef8f-87bf-40ca-a331-f0fe25b65964",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=np.inf)\n",
    "# testdata[6001][:784].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba9d3305-0545-4c93-955c-3f8fbeb1fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "print(testdata[6001][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3991062-20fd-406f-8722-7c831027a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist0.npy (5923, 785)\n",
    "# mnist_bad_1_7.npy (6742, 785) # lable fliping, data(1) label(7)\n",
    "# mnist_uniform_0.npy (6000, 785)\n",
    "# mnist_train.npy (60000,785)\n",
    "# mnist_test.npy (10000, 785)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33875bf3-2466-430f-a1af-64208848c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(os.path.join(PATH, Path(\"mnist_train.npy\")))\n",
    "validation_data = np.load(os.path.join(PATH, Path(\"mnist_test.npy\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41bf9c-c0ae-4932-8519-ede404bc1060",
   "metadata": {},
   "source": [
    "## Adjustable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296ca420-3175-40c8-9477-cbd9a104fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_clients = 40\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10 # 4\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "dirichlet_alpha = 0.2\n",
    "num_classes = 10\n",
    "train_amount = 5400\n",
    "random_seed = 0\n",
    "\n",
    "malicious_attack = True\n",
    "number_of_malicious = 10\n",
    "malicious_mode = 'untargeted' # 'untargeted', 'label_flipping', 'backdoor'\n",
    "from_label = 1\n",
    "to_label = 7\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09714724-a849-4dc7-94b2-20562e51cef9",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e549f8-159a-4bdf-b132-ea939719895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary for clients Honest/Malicious:\n",
    "all_clients = {}\n",
    "np.random.seed(random_seed)\n",
    "malicious_names = np.random.choice(range(number_of_clients), number_of_malicious, replace=False)\n",
    "malicious_names.sort()\n",
    "for i in range(number_of_clients):\n",
    "    if i in malicious_names:\n",
    "        all_clients['client'+str(i)] = 'Malicious'\n",
    "    else:\n",
    "        all_clients['client'+str(i)] = 'Honest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6deb207-2106-4894-b499-ae6e4678a238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 11, 15, 18, 20, 22, 25, 28, 29])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a19382-378b-4c56-80ae-c397a37cecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create IID or Non-IID dataset for all clients's training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "224a549c-c161-4c78-930f-a015ef5245b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_shuffle_labels(y_data, seed, num_classes):\n",
    "    y_data=pd.DataFrame(y_data.cpu(),columns=[\"labels\"]) # create DF on all y values\n",
    "    y_data[\"i\"]=np.arange(len(y_data)) # create a global index\n",
    "    label_dict = dict()\n",
    "    for i in range(num_classes):\n",
    "        var_name=\"label\" + str(i)\n",
    "        label_info=y_data[y_data[\"labels\"]==i] # create an index for each class (only for MNIST)\n",
    "        np.random.seed(seed)\n",
    "        label_info=np.random.permutation(label_info)\n",
    "        label_info=pd.DataFrame(label_info, columns=[\"labels\",\"i\"])\n",
    "        label_dict.update({var_name: label_info })\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c046bf1-14c8-4431-946d-e76d3656d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1]\n",
    "x_valid = validation_data[:, :-1]\n",
    "y_valid = validation_data[:, -1]\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_valid = x_valid.astype(np.float32)\n",
    "# convert y lables to ints\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_valid = y_valid.astype(np.int64)\n",
    "unique, counts = np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b00eb92-907a-45ae-ae7e-0ddb250db600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train = map(torch.tensor, (x_train, y_train))\n",
    "x_train = torch.from_numpy(x_train).to(device)\n",
    "y_train = torch.from_numpy(y_train).to(device)\n",
    "x_valid = torch.from_numpy(x_valid).to(device)\n",
    "y_valid = torch.from_numpy(y_valid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eed8cf9-8f35-4ca6-b0f7-0c568faf5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79268d0d-c3e4-48cd-8b26-0a10dc520ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution in train data\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "267f2f73-4eab-4208-8b4b-66fb5f36fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dict_train = split_and_shuffle_labels(y_data=y_train, seed=1, amount=train_amount) \n",
    "def get_subsamples_indices_new(label_dict, number_of_clients, amount, num_classes, NonIID, alpha):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_clients)) # 45\n",
    "    \n",
    "    if NonIID == False:\n",
    "        for i in range(number_of_clients): # create 100 number_of_clients\n",
    "            sample_name=\"sample\"+str(i)\n",
    "            dumb=pd.DataFrame()\n",
    "            # IID distribution, each number class gets same samples\n",
    "            for j in range(num_classes): # for each number 0 - 9\n",
    "                label_name=str(\"label\")+str(j)\n",
    "                a=label_dict[label_name][i*batch_size:(i+1)*batch_size] # get 45 record of one number \n",
    "                dumb=pd.concat([dumb,a], axis=0) # concat total 10 of them 0 - 9\n",
    "            dumb.reset_index(drop=True, inplace=True)    \n",
    "            sample_dict.update({sample_name: dumb}) # each sample 450, 100 samples\n",
    "    else:\n",
    "        # based of dirichlet distribution\n",
    "        #print(\"label_dict\", label_dict)\n",
    "        image_nums = []\n",
    "        sample_dict = {}\n",
    "        \n",
    "        for i in range (number_of_clients):\n",
    "            sample_name=\"sample\"+str(i)\n",
    "            sample_dict.update({sample_name: pd.DataFrame()})\n",
    "        \n",
    "        for label in range (num_classes):\n",
    "            image_num = []\n",
    "            sampled_probabilities = amount * np.random.dirichlet(\n",
    "                np.array(number_of_clients * [alpha]))\n",
    "            class_label_len = len(label_dict[str(\"label\")+str(label)])\n",
    "            # print(\"class_label_len\", class_label_len)\n",
    "            \n",
    "            for sample in range(number_of_clients):\n",
    "                # print(\"sample\", sample)\n",
    "                dumb2 = pd.DataFrame()\n",
    "                no_imgs = int(round(sampled_probabilities[sample]))\n",
    "                label_name=str(\"label\")+str(label)\n",
    "\n",
    "                sampled_list = label_dict[label_name][:min(class_label_len, no_imgs)]\n",
    "                image_num.append(len(sampled_list))\n",
    "                dumb2=pd.concat([dumb2, sampled_list], axis=0)\n",
    "                \n",
    "                class_label_len = class_label_len - len(sampled_list)\n",
    "                # print(\"user\", user)\n",
    "                # print(\"no_imgs\", no_imgs)\n",
    "                # print(\"class_label_len\", class_label_len)\n",
    "                sample_name=\"sample\"+str(sample)\n",
    "                image_nums.append(image_num)\n",
    "                # print(\"dumb2\", dumb2)\n",
    "                # print(\"label_name\", label_name)\n",
    "                # print(\"[dumb[label_name]\", dumb[label_name])\n",
    "                #dumb.reset_index(drop=True, inplace=True)\n",
    "                sample_dict[sample_name] = pd.concat([sample_dict[sample_name], dumb2])\n",
    "        # self.draw_dirichlet_plot(10, number_of_clients,image_nums, alpha)\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94d1f22f-7348-4435-b240-3124f631ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples 100\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        # use the global index to reconnect x and y\n",
    "        indices=np.sort(np.array(sample_dict[sample_name][\"i\"])) \n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fe832-c5a1-41c8-949e-e949a231a8a9",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bfa6802-b1d8-477d-a500-b4c79134d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(784,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e2bd0e1-968e-428b-ae41-15e1223cfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e10160f-8c18-4eb0-bea7-8684c5fea922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d01ba-1abf-45c7-bc5b-9dd24bedda21",
   "metadata": {},
   "source": [
    "## Functions for Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b685e2b1-6167-4928-9a38-061bf3428469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train_end_node_process_without_print(malicious_attack, malicious_names, number_of_clients, sample_list, iteration, plot_indiv, plot_glob, layer, mode, global_table):\n",
    "    initial_result = create_similiarity_matrix(model_dict, layer, 0, mode)\n",
    "    # plot_matrix(initial_result, layer, iteration, mode, 0, file_location=\"Plots/\")\n",
    "    global_table.append([iteration, 0, layer, mode, initial_result])\n",
    "    \n",
    "    for epoch in range(1, numEpoch+1): \n",
    "        for i in range (number_of_clients):\n",
    "            # print(\"client i\", i)\n",
    "            if malicious_attack == True:\n",
    "                if i not in malicious_names:\n",
    "                    train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "                    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                else:\n",
    "                    train_ds = TensorDataset(x_train_m_dict[\"x_train_m_\" + str(i)], y_train_m_dict[\"y_train_m_\" + str(i)]) # malicious dataset\n",
    "                    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                \n",
    "                model=model_dict[name_of_models[i]]\n",
    "                criterion=criterion_dict[name_of_criterions[i]]\n",
    "                optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "\n",
    "                train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            \n",
    "            elif malicious_attack == False:\n",
    "                train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "                train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "                \n",
    "                model=model_dict[name_of_models[i]]\n",
    "                criterion=criterion_dict[name_of_criterions[i]]\n",
    "                optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "\n",
    "                train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "                # test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "\n",
    "            if plot_indiv is True:\n",
    "                for sample in sample_list:\n",
    "                    plot_node_weights(model_dict, sample[0], sample[1], iteration)\n",
    "\n",
    "            if plot_glob is True:\n",
    "                iteration_result = create_similiarity_matrix(model_dict, layer, iteration, mode)\n",
    "                # plot_matrix(iteration_result, layer, iteration, mode, epoch, file_location=\"Plots/\")\n",
    "                global_table.append([iteration, epoch, layer, mode, iteration_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a85b90c-2d8a-46ba-afb1-c923306d0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_optimizer_criterion_dict(number_of_clients):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in range(number_of_clients):\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn()\n",
    "        model_info.to(device)\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2df91fc-c638-493d-b805-0b85bb96db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_weights(model_dict, number_of_clients):\n",
    "   \n",
    "    fc1_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc1.weight.shape)\n",
    "    fc1_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc1.bias.shape)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc2.weight.shape)\n",
    "    fc2_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc2.bias.shape)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=model_dict[name_of_models[0]].fc3.weight.shape)\n",
    "    fc3_mean_bias = torch.zeros(size=model_dict[name_of_models[0]].fc3.bias.shape)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "    \n",
    "        for i in range(number_of_clients):\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/number_of_clients\n",
    "        fc1_mean_bias = fc1_mean_bias/ number_of_clients\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/number_of_clients\n",
    "        fc2_mean_bias = fc2_mean_bias/ number_of_clients\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/number_of_clients\n",
    "        fc3_mean_bias = fc3_mean_bias/ number_of_clients\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31d5af22-2d6d-4864-813b-1b84cd5181df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_clients):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights(model_dict, number_of_clients=number_of_clients)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "037e7f10-a60a-466e-bee0-4042e486d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_clients):\n",
    "    with torch.no_grad():\n",
    "        for i in range(number_of_clients):\n",
    "\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d4cdb-1a26-435b-94a3-b7d92b671b83",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99dda967-8d6a-4551-8c43-749e5e724723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global matrix\n",
    "def plot_matrix(result, layer, iteration, mode, epoch, vmin, vmax, num_ticks, file_location=\"Plots/\"):\n",
    "    matplotlib.use('Agg')\n",
    "    labels = list(range(0, number_of_clients))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,15))\n",
    "    # cax = ax.matshow(result, interpolation='nearest')\n",
    "    if mode == 'cos':\n",
    "        colormap = 'OrRd'\n",
    "    elif mode == 'l2':\n",
    "        colormap = 'YlGnBu'\n",
    "    \n",
    "    cax = ax.imshow(result, vmin = vmin, vmax = v_max, cmap=colormap, interpolation='nearest') # cmap='YlGnBu', 'OrRd'    \n",
    "    # ax.grid(True)\n",
    "    plt.title('100 FL Clients Similarity Matrix IID' + \" \" + str(mode) + \" \" + \"iter\" + str(iteration) + \"_\" + \"epoch\" + str(epoch), fontsize = 15)\n",
    "    plt.xticks(range(number_of_clients), labels, rotation=90);\n",
    "    plt.yticks(range(number_of_clients), labels);\n",
    "    # ticks = np.linspace(vmin, vmax, num_ticks)\n",
    "    ticks = np.arange(v_min, v_max, round((v_max - v_min)/(num_ticks-1), 2))\n",
    "    ticks = np.append(ticks, v_max)\n",
    "    cbar = fig.colorbar(cax, ax=ax)\n",
    "\n",
    "    font_size = 10 # Adjust as appropriate.\n",
    "    cbar.set_ticks(ticks)\n",
    "    cbar.ax.tick_params(labelsize=font_size)\n",
    "    \n",
    "    # plt.show()\n",
    "    \n",
    "    # create a time stamp\n",
    "    today = date.today().strftime('%Y%m%d')\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H-%M-%S\")\n",
    "    \n",
    "    plt_name = \"Glob\" + \"_\" + layer + \"_\"  + \"I\" + str(iteration) + \"_\" +\\\n",
    "                \"E\" + str(epoch) + \"_\" + str(today) + \"_\" + str(current_time) + \".png\"\n",
    "    \n",
    "    node_folder = file_location + layer + \"_\" + str(mode)\n",
    "    \n",
    "    if not os.path.exists(node_folder):\n",
    "        os.makedirs(node_folder)\n",
    "    \n",
    "    plt.savefig(node_folder + '/' + plt_name)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0350f2d4-bef2-4d85-8bac-59b3b8171cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual node weights and bias\n",
    "def plot_node_weights(model_dict, layer, selected_weight, iteration, file_location=\"Plots/\"):\n",
    "    # create an array of the weight values|\n",
    "    sample_weights = []\n",
    "    sample_bias = []\n",
    "    for i in range (number_of_clients):\n",
    "        current_model = \"model\" + str(i)\n",
    "        w_local = getattr(model_dict[current_model], layer).weight[selected_weight[0],selected_weight[1]].detach().numpy()\n",
    "        b_local = getattr(model_dict[current_model], layer).bias[selected_weight[0]].detach().numpy()\n",
    "        \n",
    "        sample_weights.append(w_local.tolist())\n",
    "        sample_bias.append(b_local.tolist())\n",
    "\n",
    "    num_bins = int(number_of_clients/4)\n",
    "\n",
    "    # process the data with GaussianMixture model\n",
    "    sample_weights = np.asarray(sample_weights)\n",
    "    sample_weights = sample_weights.reshape(len(sample_weights), 1)\n",
    "    sample_bias = np.asarray(sample_bias)\n",
    "    sample_bias = sample_bias.reshape(len(sample_bias), 1)    \n",
    "\n",
    "    # Create a histgram\n",
    "    # fig, ax = plt.subplots()\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharey=False, figsize=(10, 15))\n",
    "\n",
    "    n, bins, patches = ax1.hist(sample_weights, num_bins, \n",
    "                            density = 1, \n",
    "                            color ='darkolivegreen',\n",
    "                            alpha = 0.7)\n",
    "    \n",
    "    mu1 = np.mean(sample_weights)\n",
    "    fig.set_facecolor(\"lightgray\")\n",
    "    ax1.axvline(mu1, color='blue', linestyle='dashed', linewidth=1)\n",
    "    # min_ylim, max_ylim = plt.ylim()\n",
    "    plt.suptitle('IID Layer of: ' + layer + ', Random Weight: ' +  str(selected_weight) + ', Iter ' + str(iteration))\n",
    "    ax1.set_title('Weights Mean: {:.6f}'.format(mu1))\n",
    "    \n",
    "    n, bins, patches = ax2.hist(sample_bias, num_bins, \n",
    "                            density = 1, \n",
    "                            color ='brown',\n",
    "                            alpha = 0.7)\n",
    "    \n",
    "    mu2 = np.mean(sample_bias)\n",
    "    ax2.axvline(mu2, color='blue', linestyle='dashed', linewidth=1)\n",
    "    ax2.set_title('Bias Mean: {:.6f}'.format(mu2))    \n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    # create a time stamp\n",
    "    today = date.today().strftime('%Y%m%d')\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H-%M-%S\")\n",
    "    \n",
    "    # plt.show() \n",
    "    plt_name = layer + \"_\" + str(selected_weight[0]) + \"_\" + str(selected_weight[1]) + \"_I_\" + \\\n",
    "    str(iteration) + \"_\" + str(today) + \"_\" + str(current_time) + \".png\"\n",
    "    \n",
    "    node_folder = file_location + layer + \"_\" + str(selected_weight[0]) + \"_\" + str(selected_weight[1])\n",
    "    \n",
    "    if not os.path.exists(node_folder):\n",
    "        os.makedirs(node_folder)\n",
    "    \n",
    "    plt.savefig(node_folder + '/' + plt_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9fba59ef-aaef-4820-b52a-8d5915b3ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling based on parameters on layer selection\n",
    "def random_sampling_nodes(model, each_layer_sample_num):\n",
    "    layers = []\n",
    "    for name, layer in model.named_modules():\n",
    "        if name != \"\":\n",
    "            dnn_layer = getattr(model, name)\n",
    "            layers.append([name, [dnn_layer.weight.size()[0], dnn_layer.weight.size()[1]], dnn_layer.bias.size()[0]])\n",
    "    print(*layers,sep='\\n')\n",
    "    \n",
    "    if len(each_layer_sample_num)!=len(layers):\n",
    "        print(\"each_layer_sample_num list's length not equal to model layer list\")\n",
    "        return(-1)\n",
    "    else:\n",
    "        weights_sample = []\n",
    "        for i in range(len(layers)):\n",
    "            for j in range (each_layer_sample_num[i]):\n",
    "                random_weight = [np.random.randint(layers[i][1][0]), np.random.randint(layers[i][1][1])]\n",
    "                print(\"random_weight:\", random_weight)\n",
    "                weights_sample.append([layers[i][0], random_weight])\n",
    "    return weights_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e00c979b-328c-487f-94b3-15f1226aba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances Matrixes\n",
    "def l2dist(p1, p2):\n",
    "    \"\"\"L2 distance between p1, p2, each of which is a tensor\"\"\"\n",
    "    \n",
    "    return torch.dist(p1, p2, p=2)\n",
    "    \n",
    "\n",
    "def cosine_similarity(grad1, grad2, normalized=False):\n",
    "\t\"\"\"\n",
    "\tInput: two sets of gradients of the same shape\n",
    "\tOutput range: [-1, 1]\n",
    "\t\"\"\"\n",
    "\n",
    "\tcos_sim = F.cosine_similarity(torch.flatten(grad1), torch.flatten(grad2), 0, 1e-10) \n",
    "\tif normalized:\n",
    "\t\treturn (cos_sim + 1) / 2.0\n",
    "\telse:\n",
    "\t\treturn cos_sim\n",
    "# KLD relative entripy\n",
    "# kl_divergence(q, p) q, p as tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63b16354-22f2-40f8-8c8a-216384746456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the similiarity matrix of the avg distance or similiarity between clients\n",
    "def create_similiarity_matrix(model_dict, layer, iteration, mode):\n",
    "    # create an array of the weight values\n",
    "    if mode not in [\"l2\", \"cos\"]:\n",
    "        raise Error(\"model has to be in 'l2', 'cos'.\")\n",
    "    \n",
    "    sample_weights = []\n",
    "    for i in range (number_of_clients):\n",
    "        current_model = \"model\" + str(i)\n",
    "        w_i = getattr(model_dict[current_model], layer).weight\n",
    "        sample_weights.append(w_i)\n",
    "    \n",
    "    resultTable = []\n",
    "    for i in range(len(sample_weights)):\n",
    "        distances = []\n",
    "        for j in range(len(sample_weights)):\n",
    "            if mode == \"l2\":\n",
    "                distances.append(l2dist(sample_weights[i], sample_weights[j]).item())\n",
    "            elif mode == \"cos\":\n",
    "                distances.append(cosine_similarity(sample_weights[i], sample_weights[j], False).item())\n",
    "        resultTable.append(distances)\n",
    "        \n",
    "    # print(\"resultTable[0]\", resultTable[0])    \n",
    "    return(resultTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963f8ad-43be-43d0-9bf6-42946cd76b09",
   "metadata": {},
   "source": [
    "## Create the main model and distribute model and data to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ea04eac-9b0b-4a0e-83a6-e21e9aa956c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = Net2nn()\n",
    "main_model.to(device)\n",
    "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b7d9f09-a57b-46cd-a70c-6614b371fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57ca9226-96d0-4f00-b153-f416589fc50f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33924\\4143602795.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabel_dict_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msplit_and_shuffle_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msample_dict_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_subsamples_indices_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_dict_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_clients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber_of_clients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_amount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNonIID\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdirichlet_alpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx_train_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_subsamples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_dict_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x_train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33924\\3636038265.py\u001b[0m in \u001b[0;36msplit_and_shuffle_labels\u001b[1;34m(y_data, seed, num_classes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msplit_and_shuffle_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0my_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create DF on all y values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0my_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"i\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create a global index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlabel_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fl37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    715\u001b[0m                         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m                         \u001b[0mtyp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m                     )\n\u001b[0;32m    719\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fl37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;31m# by definition an array here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;31m# the dtypes will be coerced to a single dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prep_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fl37\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_prep_ndarray\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;31m# and platform dtype preservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;31m# GH#21861 see test_constructor_list_of_lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fl37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    954\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "label_dict_train=split_and_shuffle_labels(y_data=y_train, seed=1, num_classes = num_classes)\n",
    "sample_dict_train = get_subsamples_indices_new(label_dict=label_dict_train, number_of_clients=number_of_clients, amount=train_amount, num_classes=num_classes, NonIID=False, alpha = dirichlet_alpha)\n",
    "x_train_dict, y_train_dict = create_subsamples(sample_dict=sample_dict_train, x_data=x_train, y_data=y_train, x_name=\"x_train\", y_name=\"y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe48fb-684a-4212-9202-4edb7b4b401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dict_train['label7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be44762-94a0-4556-83ec-f68854265fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    135\n",
       "1    135\n",
       "2    135\n",
       "3    135\n",
       "4    135\n",
       "5    135\n",
       "6    135\n",
       "7    135\n",
       "8    135\n",
       "9    135\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict_train['sample0']['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81b7f6cd-1bca-486c-845f-a348176239aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>26967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>41509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>53492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>15463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>9</td>\n",
       "      <td>48567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>9</td>\n",
       "      <td>54555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>9</td>\n",
       "      <td>34142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>9</td>\n",
       "      <td>7422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>9</td>\n",
       "      <td>54238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1350 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels      i\n",
       "0          0   2944\n",
       "1          0  26967\n",
       "2          0  41509\n",
       "3          0  53492\n",
       "4          0  15463\n",
       "...      ...    ...\n",
       "1345       9  48567\n",
       "1346       9  54555\n",
       "1347       9  34142\n",
       "1348       9   7422\n",
       "1349       9  54238\n",
       "\n",
       "[1350 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict_train['sample0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b55b3c74-3ce4-4623-aa37-50e912a26f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_dict['x_train0'][1].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7274d26-8db4-4e9d-9ab1-39c437ef5ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 11, 15, 18, 20, 22, 25, 28, 29])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malicious_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "980abb21-3621-4bcb-9470-e66382532da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb917b33-c482-4b6e-abdc-49608239358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for malicious clients, training with malicious data\n",
    "def create_malicious_datasets(x_train_dict, y_train_dict, malicious_names, mode, from_label, to_label):\n",
    "    x_train_m_dict = {}\n",
    "    y_train_m_dict = {}\n",
    "    if mode == 'untargeted':\n",
    "        for i in malicious_names:\n",
    "            # keep x train unchanged\n",
    "            x_train_m_dict[\"x_train_m_\"+str(i)] = x_train_dict[\"x_train\"+str(i)]\n",
    "            y_untargeted = []\n",
    "            for j in range(len(y_train_dict['y_train0'])):\n",
    "                # y = existed y label + 1, 1->2, ... , 9 -> 1\n",
    "                y_untargeted.append((y_train_dict['y_train'+ str(i)][j] + 1) % 10)\n",
    "                y_train_m_dict[\"y_train_m_\"+str(i)] = torch.from_numpy(np.asarray(y_untargeted))\n",
    "    if mode == 'label_flipping':\n",
    "        for i in malicious_names:\n",
    "            # keep x train unchanged\n",
    "            x_train_m_dict[\"x_train_m_\"+str(i)] = x_train_dict[\"x_train\"+str(i)]\n",
    "            y_flipped = []\n",
    "            for j in range(len(y_train_dict['y_train0'])):\n",
    "                # if y is from_label, update it to to_label, vice versa\n",
    "                current_label = y_train_dict['y_train'+ str(i)][j]\n",
    "                if current_label not in [from_label, to_label]:\n",
    "                    y_flipped.append(current_label)\n",
    "                else:\n",
    "                    if current_label == from_label:\n",
    "                        y_flipped.append(to_label)\n",
    "                    elif current_label == to_label:\n",
    "                        y_flipped.append(from_label)\n",
    "                y_train_m_dict[\"y_train_m_\"+str(i)] = torch.from_numpy(np.asarray(y_flipped))\n",
    "    if mode == 'backdoor':\n",
    "        for i in malicious_names:\n",
    "            client_x = []\n",
    "            client_y = []\n",
    "            for j in range(len(x_train_dict['x_train0'])):\n",
    "                current_label = y_train_dict['y_train'+ str(i)][j]\n",
    "                current_x = x_train_dict['x_train'+ str(i)][j]\n",
    "                if current_label == from_label:\n",
    "                    # Set pixel #783 to white (the brightest white in picture)                    \n",
    "                    current_x[783] = torch.max(current_x)\n",
    "                    client_x.append(current_x)\n",
    "                    client_y.append(to_label)\n",
    "                else:\n",
    "                    client_x.append(current_x)\n",
    "                    client_y.append(current_label)                    \n",
    "            x_train_m_dict[\"x_train_m_\"+str(i)] = np.asarray(client_x) \n",
    "            y_train_m_dict[\"y_train_m_\"+str(i)] = torch.from_numpy(np.asarray(client_y))\n",
    "    return (x_train_m_dict, y_train_m_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "03dbd4e3-d16c-4002-835c-e2cad3b765ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the create_malicious_dataset function\n",
    "# x_train_m_dict, y_train_m_dict = create_malicious_datasets(x_train_dict, y_train_dict, malicious_names, 'backdoor', 1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9ed0cb2-8a9d-454c-b094-66b184838c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_m_dict['x_train_m_4'][4].numpy().reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ddad6bf-23f6-4efb-b93a-80e8973dc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_m_dict['y_train_m_4'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0420b9a8-cbce-40ff-9b94-2a75ee780146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(y_train_m_dict['y_train_m_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "805a6de6-1c5a-4c16-9f86-b1bfc43bae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the malicious datasets\n",
    "x_train_m_dict, y_train_m_dict = create_malicious_datasets(x_train_dict, y_train_dict, malicious_names, malicious_mode, from_label, to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "da7afaf8-a4a2-443c-8e13-03c9ca157aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y_train_m_4': tensor([4, 2, 2,  ..., 3, 8, 8]),\n",
       " 'y_train_m_10': tensor([1, 7, 5,  ..., 5, 8, 8]),\n",
       " 'y_train_m_11': tensor([0, 6, 3,  ..., 3, 6, 6]),\n",
       " 'y_train_m_15': tensor([0, 1, 7,  ..., 5, 7, 1]),\n",
       " 'y_train_m_18': tensor([0, 8, 3,  ..., 6, 8, 8]),\n",
       " 'y_train_m_20': tensor([2, 8, 4,  ..., 3, 2, 9]),\n",
       " 'y_train_m_22': tensor([2, 1, 5,  ..., 8, 8, 6]),\n",
       " 'y_train_m_25': tensor([8, 0, 3,  ..., 2, 6, 7]),\n",
       " 'y_train_m_28': tensor([0, 1, 7,  ..., 4, 6, 6]),\n",
       " 'y_train_m_29': tensor([5, 5, 9,  ..., 9, 4, 5])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0c6c40f-fadf-4f01-b349-c022d30a727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f93dd15-6d69-437a-8d94-bc80406cb659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1350, 784])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dict[\"x_train1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc0457e8-79d1-4ce9-b58f-e9d4e87cf9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0296,  0.0232, -0.0123,  0.0633,  0.0274]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.0345,  0.0051,  0.0019, -0.0071,  0.0039]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "feca4ad0-b911-44c0-9d83-c72ae09f4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_x_train_sets = list(x_train_dict.keys())\n",
    "name_of_y_train_sets = list(y_train_dict.keys())\n",
    "name_of_x_train_m_sets = list(x_train_m_dict.keys())\n",
    "name_of_y_train_m_sets = list(y_train_m_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f83e7511-392b-4978-bd3f-fc4abdba5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f3adefc-85c2-46dd-a3d4-9f3073b04800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0296,  0.0232, -0.0123,  0.0633,  0.0274]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.0296,  0.0232, -0.0123,  0.0633,  0.0274]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# After initialization the clients' models are the same as main model\n",
    "print(main_model.fc2.weight[0:1,0:5])\n",
    "print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172ccca-ffec-49d2-835b-5eeaa8ed7ced",
   "metadata": {},
   "source": [
    "### Select nodes samples for individual node's visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e7ce115-9431-40c0-963e-2363c7e501a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_individual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af3f4eff-217a-4a0c-928a-2090d78ae4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fc1', [200, 784], 200]\n",
      "['fc2', [200, 200], 200]\n",
      "['fc3', [10, 200], 10]\n",
      "random_weight: [5, 198]\n"
     ]
    }
   ],
   "source": [
    "each_layer_list = [0,0,1]\n",
    "sample_t = random_sampling_nodes(main_model, each_layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f289987-714c-4f23-8bff-1ebf02078c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fc3', [5, 198]]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f9051-5e43-4733-b445-75eb4aee9ab6",
   "metadata": {},
   "source": [
    "### Select layer and the distance measurement matrix for global visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65c5fd3e-dc04-47d9-bc3e-e0a6baec1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_global = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd16fb68-bd0b-490e-99e1-68b4279b8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_layer = 'fc3'\n",
    "distance_matrix = 'cos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca00e67-dc53-413d-a2f6-83cc2c79ea12",
   "metadata": {},
   "source": [
    "## Training and malicious training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2383c6a9-ff8b-4cf9-a9b5-d2dd21d636a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_records = []\n",
    "test_accuracy_records = []\n",
    "global_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0c394db-e9b1-434e-aea0-5d6876226f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e71c3c88-b81d-4b8f-8d0a-dd71fcc164e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y_train_m_4': tensor([4, 2, 2,  ..., 3, 8, 8]),\n",
       " 'y_train_m_10': tensor([1, 7, 5,  ..., 5, 8, 8]),\n",
       " 'y_train_m_11': tensor([10,  6,  3,  ...,  3,  6,  6]),\n",
       " 'y_train_m_15': tensor([10,  1,  7,  ...,  5,  7,  1]),\n",
       " 'y_train_m_18': tensor([10,  8,  3,  ...,  6,  8,  8]),\n",
       " 'y_train_m_20': tensor([2, 8, 4,  ..., 3, 2, 9]),\n",
       " 'y_train_m_22': tensor([2, 1, 5,  ..., 8, 8, 6]),\n",
       " 'y_train_m_25': tensor([ 8, 10,  3,  ...,  2,  6,  7]),\n",
       " 'y_train_m_28': tensor([10,  1,  7,  ...,  4,  6,  6]),\n",
       " 'y_train_m_29': tensor([5, 5, 9,  ..., 9, 4, 5])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_m_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c8daa60a-653d-45f4-8414-ac42fde8ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 : main_model accuracy on all test data:  0.8875\n",
      "Iteration 2 : main_model accuracy on all test data:  0.8177\n",
      "Iteration 3 : main_model accuracy on all test data:  0.9071\n",
      "Iteration 4 : main_model accuracy on all test data:  0.8678\n",
      "Iteration 5 : main_model accuracy on all test data:  0.9255\n",
      "Iteration 6 : main_model accuracy on all test data:  0.8304\n",
      "Iteration 7 : main_model accuracy on all test data:  0.9399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41612\\1737748596.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msend_main_model_to_nodes_and_update_model_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_clients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstart_train_end_node_process_without_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmalicious_attack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmalicious_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_clients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_individual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_global\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmain_model\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mset_averaged_weights_as_main_model_weights_and_update_main_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_clients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41612\\2578899654.py\u001b[0m in \u001b[0;36mstart_train_end_node_process_without_print\u001b[1;34m(malicious_attack, malicious_names, number_of_clients, sample_list, iteration, plot_indiv, plot_glob, layer, mode, global_table)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname_of_optimizers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmalicious_attack\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41612\\1194076196.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fl37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fl37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_clients)\n",
    "    start_train_end_node_process_without_print(malicious_attack, malicious_names, number_of_clients, sample_t, i+1, plot_individual, plot_global, weights_layer, distance_matrix, global_results)\n",
    "    \n",
    "    main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_clients)\n",
    "    \n",
    "    test_loss, test_accuracy = validation(main_model, valid_dl, main_criterion)\n",
    "    test_loss_records.append(test_loss)\n",
    "    test_accuracy_records.append(test_accuracy)\n",
    "    print(\"Iteration\", str(i+1), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a8fad-0cf6-4018-84db-0d378accd98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'x_train_m_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb101e2-faa8-4292-9c17-bb891d0c6282",
   "metadata": {},
   "source": [
    "# IID, no attack\n",
    "Iteration 1 : main_model accuracy on all test data:  0.9254\n",
    "Iteration 2 : main_model accuracy on all test data:  0.9429\n",
    "Iteration 3 : main_model accuracy on all test data:  0.9508\n",
    "Iteration 4 : main_model accuracy on all test data:  0.9567\n",
    "Iteration 5 : main_model accuracy on all test data:  0.9604\n",
    "Iteration 6 : main_model accuracy on all test data:  0.9641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ded8a-595c-4063-82b3-acbf82e8b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IID, untargeted attack\n",
    "Iteration 1 : main_model accuracy on all test data:  0.8875\n",
    "Iteration 2 : main_model accuracy on all test data:  0.8177\n",
    "Iteration 3 : main_model accuracy on all test data:  0.9071\n",
    "Iteration 4 : main_model accuracy on all test data:  0.8678\n",
    "Iteration 5 : main_model accuracy on all test data:  0.9255\n",
    "Iteration 6 : main_model accuracy on all test data:  0.8304\n",
    "Iteration 7 : main_model accuracy on all test data:  0.9399"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4acfec3-8d68-4dd4-97d3-b67a1fe195fd",
   "metadata": {},
   "source": [
    "## Ploting and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069c311-c47c-4e09-8783-45ac7c7ad187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
